# -*- coding: utf-8 -*-
"""OD-ACTravel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZPnADdG3lbF_9Cjh7QTQcQWf9XHqtn30
"""

pip install scikit-mobility

import skmob
from skmob.utils import utils, constants
import geopandas as gpd
import pandas as pd
import numpy as np
import networkx as nx
import json

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
try:
    import powerlaw
    import mpmath
except:
    !pip install powerlaw
    !pip install mpmath
    import mpmath
    import powerlaw
import scipy as sp
import geopy.distance
import seaborn as sns
# %matplotlib inline

"""#  1 Import and Combine Data"""

from google.colab import drive
drive.mount('/content/drive/')

#O-D by Trip Origin: Replica's weekly mobility data provides near-real-time insight into mobility patterns for each census tract in the country. This dataset contains information about trip origin-destination volumes for trips starting in the selected geographic area, for the selected time period.
fd_1=pd.read_csv('/content/drive/MyDrive/C257 Project/Data/OD_2023-02-04_to_2023-06-02-weekday.csv')
fd_2=pd.read_csv('/content/drive/MyDrive/C257 Project/Data/OD_2023-02-04_to_2023-06-02-weekend.csv')

fd_3=pd.read_csv('/content/drive/MyDrive/C257 Project/Data/OD_2023-06-01_to_2023-09-30-weekend.csv')
fd_4=pd.read_csv('/content/drive/MyDrive/C257 Project/Data/OD_2023-06-03_to_2023-09-29-weekday.csv')

# Drop the columns
fd_1d=fd_1.drop(columns=['other_travel_mode_trip_count','public_transit_trip_count','private_auto_trip_count','on_demand_auto_trip_count'
                ,'auto_passenger_trip_count'])

fd_2d=fd_2.drop(columns=['other_travel_mode_trip_count','public_transit_trip_count','private_auto_trip_count','on_demand_auto_trip_count'
                ,'auto_passenger_trip_count'])
fd_3d=fd_3.drop(columns=['other_travel_mode_trip_count','public_transit_trip_count','private_auto_trip_count','on_demand_auto_trip_count'
                ,'auto_passenger_trip_count'])
fd_4d=fd_4.drop(columns=['other_travel_mode_trip_count','public_transit_trip_count','private_auto_trip_count','on_demand_auto_trip_count'
                ,'auto_passenger_trip_count'])

# Combine fds together: San Francisco walking biking travel data from Feb to Sep 2023
fd_all = pd.concat([fd_1d, fd_2d, fd_3d, fd_4d], ignore_index=True)

#Rename and drop
fd_all = fd_all.drop(columns=['week_starting','origin_geo_name','destination_geo_name']).rename(columns={'origin_geo_id':'origin','destination_geo_id':'destination'})

fd_all = fd_all.dropna()

fd_all

# Group by origin and denstination
fd_group = fd_all.groupby(['origin','destination']).agg({
    'origin_population': 'first',
    'destination_population': 'first',
    'walking_trip_count': 'sum',
    'biking_trip_count': 'sum'
}).reset_index()
fd_group

fd_group.to_csv('/content/drive/MyDrive/C257 Project/Data/OD-ACTravel.csv', index=False)

# load a tessellation
tessellation = gpd.GeoDataFrame.from_file(
    "/content/drive/MyDrive/C257 Project/Data/2010_sf_census_tracts.geojson")

tessellation['GEOID10']=tessellation['GEOID10'].str[1:].astype(int)

# Test if origin and destination IDs are present in the tessellation
fd_group['origin'].isin(tessellation['GEOID10']).unique()

fd_group['destination'].isin(tessellation['GEOID10']).unique()

# Drop the rows whose destination code is not in tessellation
fd_group=fd_group[fd_group['destination'].isin(tessellation['GEOID10'])]

fd_group

# Create flowdataframe for walking
fdf_walking = skmob.FlowDataFrame(
    fd_group,origin='origin', destination='destination',flow='walking_trip_count',tessellation=tessellation,tile_id='GEOID10')

# Create flowdataframe for biking
fdf_biking = skmob.FlowDataFrame(
    fd_group,origin='origin', destination='destination',flow='biking_trip_count',tessellation=tessellation,tile_id='GEOID10')

fdf_walking = fdf_walking.drop(columns=['biking_trip_count'])
fdf_biking = fdf_biking.drop(columns=['walking_trip_count'])

tessellation['GEOID10']=tessellation['GEOID10'].astype(str)

tessellation = tessellation.merge(fdf_walking[['origin','origin_population']],left_on='GEOID10',right_on='origin')
tessellation

tessellation = tessellation.rename({'origin_population':'population'})

fdf_walking

fdf_biking

"""# 2 Plot"""

import folium

# Initialize a map centered at [53, 12] at zoom level 5
m = folium.Map(location=[37.7749, -122.4194], zoom_start=12,)

# Add Stamen Toner tile layer
folium.raster_layers.TileLayer(
    tiles='https://tiles.stadiamaps.com/tiles/stamen_toner_background/{z}/{x}/{y}{r}.png',
    attr='&copy; <a href="https://stadiamaps.com/" target="_blank">Stadia Maps</a> &copy; <a href="https://stamen.com/" target="_blank">Stamen Design</a> &copy; <a href="https://openmaptiles.org/" target="_blank">OpenMapTiles</a> &copy; <a href="https://www.openstreetmap.org/about" target="_blank">OpenStreetMap</a> contributors',
    max_zoom=20,
    name='Stamen Toner',
    overlay=False,
    control=True
).add_to(m)

# Display the map
m

# Initialize a map centered at [37.7749, -122.4194] at zoom level 12
m2 = folium.Map(location=[37.7749, -122.4194], zoom_start=12,)

# Add Stamen Toner tile layer
folium.raster_layers.TileLayer(
    tiles='https://tiles.stadiamaps.com/tiles/stamen_toner_background/{z}/{x}/{y}{r}.png',
    attr='&copy; <a href="https://stadiamaps.com/" target="_blank">Stadia Maps</a> &copy; <a href="https://stamen.com/" target="_blank">Stamen Design</a> &copy; <a href="https://openmaptiles.org/" target="_blank">OpenMapTiles</a> &copy; <a href="https://www.openstreetmap.org/about" target="_blank">OpenStreetMap</a> contributors',
    max_zoom=20,
    name='Stamen Toner',
    overlay=False,
    control=True
).add_to(m2)

# Initialize a map centered at [37.7749, -122.4194] at zoom level 12
m3 = folium.Map(location=[37.7749, -122.4194], zoom_start=12,)

# Add Stamen Toner tile layer
folium.raster_layers.TileLayer(
    tiles='https://tiles.stadiamaps.com/tiles/stamen_toner_background/{z}/{x}/{y}{r}.png',
    attr='&copy; <a href="https://stadiamaps.com/" target="_blank">Stadia Maps</a> &copy; <a href="https://stamen.com/" target="_blank">Stamen Design</a> &copy; <a href="https://openmaptiles.org/" target="_blank">OpenMapTiles</a> &copy; <a href="https://www.openstreetmap.org/about" target="_blank">OpenStreetMap</a> contributors',
    max_zoom=20,
    name='Stamen Toner',
    overlay=False,
    control=True
).add_to(m3)

# Assuming plot_tessellation is a method that adds layers to the map
fdf_walking.plot_tessellation(map_f=m, popup_features=['GEOID10', 'population'])

fdf_biking.plot_flows(flow_color='red',flow_weight=2, radius_origin_point=2, color_origin_point='red',opacity=0.1,map_f=m3)

fdf_walking.plot_flows(flow_color='red',flow_weight=2, radius_origin_point=2, color_origin_point='red',opacity=0.1,map_f=m2)

"""# 3 Flow Analysis"""

# compute the total outflows from each location of the tessellation (excluding self loops)
walking_outflows = fdf_walking[fdf_walking['origin'] != fdf_walking['destination']].groupby(by='origin', axis=0)['flow'].sum().fillna(0).reset_index()
walking_inflows = fdf_walking[fdf_walking['origin'] != fdf_walking['destination']].groupby(by='destination', axis=0)['flow'].sum().fillna(0).reset_index()

walking_outflows

walking_inflows

sum(walking_inflows['flow'])

sum(walking_outflows['flow'])

tessellation['GEOID10'] = tessellation['GEOID10'].astype(str)
walking_outflows['origin'] = walking_outflows['origin'].astype(str)

tessellation_2 = pd.merge(tessellation,walking_outflows, left_on='GEOID10', right_on='origin', how='inner').rename(columns={'flow':constants.TOT_OUTFLOW})

tessellation_2=tessellation_2.drop_duplicates()

"""## 4 Fit Constrained GM"""

from skmob.utils import utils, constants
from skmob.models.gravity import Gravity

np.random.seed(10)
gravity_singly_fitted = Gravity(gravity_type='singly constrained',deterrence_func_type="exponential")
gravity_globally_fitted = Gravity(gravity_type='globally constrained',deterrence_func_type="power_law")

print(gravity_singly_fitted)
print(gravity_globally_fitted)

walk_fdf_singly = gravity_singly_fitted.generate(tessellation_2,tile_id_column='GEOID10',tot_outflows_column='tot_outflow',relevance_column='origin_population',out_format='flows')

walk_fdf_globally = gravity_globally_fitted.generate(tessellation_2,tile_id_column='GEOID10',tot_outflows_column='tot_outflow',relevance_column='origin_population',out_format='flows')

"""## Visualiza"""

import matplotlib.pyplot as plt
import numpy as np

# Assuming fdf and synth_fdf_singly are already defined and have the columns 'origin', 'destination', 'flow_x', and 'flow_y'
xy = fdf_walking.merge(walk_fdf_singly, on=['origin', 'destination'])[['flow_x', 'flow_y']].values

# Apply alpha blending for the scatter plot to reduce overplotting
plt.scatter(xy[:,0], xy[:,1], alpha=0.1, s=5, label='Singly Constrained Gravity')

# Generate a log-spaced array for x to plot the identity line
x = np.logspace(0, np.log10(np.max(xy)), num=100)
plt.plot(x, x, '--k', label='Identity line')

# Set the scale of both axes to log to properly show the identity line
plt.xscale('log')
plt.yscale('log')

# Set labels and legend
plt.xlabel('Empirical flow')
plt.ylabel('Model flow')
plt.legend(loc='upper left')

# Show the plot
plt.show()

# Assuming fdf and synth_fdf_singly are already defined and have the columns 'origin', 'destination', 'flow_x', and 'flow_y'
xy2 = fdf_walking.merge(walk_fdf_globally, on=['origin', 'destination'])[['flow_x', 'flow_y']].values

# Apply alpha blending for the scatter plot to reduce overplotting
plt.scatter(xy2[:,0], xy2[:,1], alpha=0.1, s=5, label='Globally Constrained Gravity')

# Generate a log-spaced array for x to plot the identity line
x = np.logspace(0, np.log10(np.max(xy)), num=100)
plt.plot(x, x, '--k', label='Identity line')

# Set the scale of both axes to log to properly show the identity line
plt.xscale('log')
plt.yscale('log')

# Set labels and legend
plt.xlabel('Empirical flow')
plt.ylabel('Model flow')
plt.legend(loc='upper left')

# Show the plot
plt.show()

"""β = 0.3 < S > ^-0.18 = 0.3*25968^ -0.18"""

β = 0.3 * (25968 ** (-0.18))
β

"""## Q6 Radiation Model"""

from skmob.models.radiation import Radiation
np.random.seed(10)
radiation = Radiation()
walk_rad_fdf = radiation.generate(tessellation_2, tile_id_column='GEOID10',  tot_outflows_column='tot_outflow',
                             relevance_column='origin_population', out_format='flows_sample')

# Assuming fdf and synth_fdf_singly are already defined and have the columns 'origin', 'destination', 'flow_x', and 'flow_y'
xyr = fdf_walking.merge(walk_rad_fdf, on=['origin', 'destination'])[['flow_x', 'flow_y']].values

# Apply alpha blending for the scatter plot to reduce overplotting
plt.scatter(xyr[:,0], xyr[:,1], alpha=0.1, s=5, label='Radiation Model')

# Generate a log-spaced array for x to plot the identity line
x = np.logspace(0, np.log10(np.max(xy)), num=100)
plt.plot(x, x, '--k', label='Identity line')

# Set the scale of both axes to log to properly show the identity line
plt.xscale('log')
plt.yscale('log')

# Set labels and legend
plt.xlabel('Empirical flow')
plt.ylabel('Model flow')
plt.legend(loc='upper left')

# Show the plot
plt.show()

"""## Q7 CPC, RMSE, R-squared value, Max error"""

from skmob.measures.evaluation import r_squared,rmse, pearson_correlation,common_part_of_commuters,max_error,common_part_of_commuters_distance

#Singly Constrained Gravity
CPC=2.0 * np.sum(np.minimum(xy[:, 0], xy[:, 1])) / (np.sum(xy[:, 0]) + np.sum(xy[:, 1]))
RMSE = np.sqrt(sum((xy[:, 0] - xy[:, 1])**2)/xy[:,0].size)
R2 = (np.corrcoef(xy[:, 0],xy[:, 1]))[0][1]**2
max_error = max(abs(xy[:,0] - xy[:,1]))

print('CPC =', CPC)
print('RMSE =', RMSE)
print('R2 =', R2)
print('Max Error =', max_error)

#Globally Constrained Gravity
CPC_g=2.0 * np.sum(np.minimum(xy2[:, 0], xy2[:, 1])) / (np.sum(xy2[:, 0]) + np.sum(xy2[:, 1]))
RMSE_g = np.sqrt(sum((xy2[:, 0] - xy2[:, 1])**2)/xy2[:,0].size)
R2_g = (np.corrcoef(xy2[:, 0],xy2[:, 1]))[0][1]**2
max_error_g = max(abs(xy2[:,0] - xy2[:,1]))

print('CPC =', CPC_g)
print('RMSE =', RMSE_g)
print('R2 =', R2_g)
print('Max Error =', max_error_g)

#Radiation Model
CPC_r=2.0 * np.sum(np.minimum(xyr[:, 0], xyr[:, 1])) / (np.sum(xyr[:, 0]) + np.sum(xyr[:, 1]))
RMSE_r = np.sqrt(sum((xyr[:, 0] - xyr[:, 1])**2)/xyr[:,0].size)
R2_r = (np.corrcoef(xyr[:, 0],xyr[:, 1]))[0][1]**2
max_error_r = max(abs(xyr[:,0] - xyr[:,1]))

print('CPC =', CPC_r)
print('RMSE =', RMSE_r)
print('R2 =', R2_r)
print('Max Error =', max_error_r)

"""# Part2
# Weighted Network

## Q1calculate the degree distributions
"""

nodes_em = nodes.set_index('county ID').drop(columns=['population', 'intra county trips', 'geometry', 'origin', 'tot_outflow'])
nodes_em.to_csv('/Users/mac/Desktop/Desktop - mac的MacBook Pro/C257/Assignment4/nodes_em.csv')

links_em = links.rename(columns={'origin county':'source','destination county':'target','number of trips':'weight'}).set_index('source')
links_em.to_csv('/Users/mac/Desktop/Desktop - mac的MacBook Pro/C257/Assignment4/links_em.csv')

links_rad=rad_fdf.rename(columns={'origin':'source','destination':'target','flow':'weight'}).set_index('source')
links_rad.to_csv('/Users/mac/Desktop/Desktop - mac的MacBook Pro/C257/Assignment4/links_rad.csv')

def makeNetworkFromFiles(fedges, fnodes,deli):
    ''' Generate a network from csv files.
    The first column of the nodes file is taken to be the id by default.
    The nodes file must contain columns named weight, source, and target.
    The deli parameter is the delimeter of the file. '''
    G = nx.Graph()  #modified from lecture 12
    fn=pd.read_csv(fnodes,delimiter=deli,index_col=0).transpose()
    n_attr=len(fn.index.values)
    attr=fn.index.values
    for n in fn.columns:
        attr_node=dict(list(zip(attr, fn[n].values)))
        G.add_node(n,attr_dic=attr_node)
    fl=pd.read_csv(fedges,delimiter=deli)
    fl.columns = list(map(str.lower, fl.columns))
    for L in fl.index.values:
        G.add_edge(fl['source'][L],fl['target'][L],weight=fl['weight'][L])
    #GL = max(nx.connected_component_subgraphs(G), key=len) ## no implemented for directed graph
    return G

def remove_values_from_list(the_list, val):
    return [value for value in the_list if value != val]

"""#### Original Data"""

#data = np.array(pd.read_csv('THE_LINKS.txt',sep=' ',header=0))
#g = nx.DiGraph()
#for row in data:
 #   g.add_edge(row[0],row[1],weight=row[3])
degrees = []
g= makeNetworkFromFiles('links_em.csv', 'nodes_em.csv',',')

degrees=list(dict(g.degree()).values())
degrees=remove_values_from_list(degrees,0)

#plt.hist(out_weighted,log=True)
#plt.yscale('log')
#plt.xscale('log')
fig, ax = plt.subplots()
fig.set_size_inches((9, 7))

n_bins = 20

#n, bins = np.histogram(out_d, bins = range(min(out_d), max(out_d)+1, 2), normed="True")
logBins = np.logspace(np.log10(min(degrees)), np.log10(max(degrees)),num=n_bins)
logBinDensity, binedges = np.histogram(degrees, bins=logBins, density=True)

ax.loglog(logBins[:-1],logBinDensity,'o', markersize=10,label=r'$k$')
ax.legend(fontsize=30)
plt.title('Empirical Data',fontsize=20)


ax.set_xlabel('$degree, k$',fontsize=30)
ax.set_ylabel('$P(k)$',fontsize=30)
plt.savefig("distributions.eps",dpi=200,bbox_inches='tight')

"""#### Radiation Model"""

#data = np.array(pd.read_csv('THE_LINKS.txt',sep=' ',header=0))
#g = nx.DiGraph()
#for row in data:
 #   g.add_edge(row[0],row[1],weight=row[3])
degreesr = []
gr= makeNetworkFromFiles('links_rad.csv', 'nodes_em.csv',',')

degreesr=list(dict(gr.degree()).values())
degreesr=remove_values_from_list(degreesr,0)

#plt.hist(out_weighted,log=True)
#plt.yscale('log')
#plt.xscale('log')
fig, ax = plt.subplots()
fig.set_size_inches((9, 7))

n_bins = 20

#n, bins = np.histogram(out_d, bins = range(min(out_d), max(out_d)+1, 2), normed="True")
logBinsr = np.logspace(np.log10(min(degreesr)), np.log10(max(degreesr)),num=n_bins)
logBinDensityr, binedgesr = np.histogram(degreesr, bins=logBinsr, density=True)

ax.loglog(logBinsr[:-1],logBinDensityr,'o', markersize=10,label=r'$k$')
ax.legend(fontsize=30)
plt.title('Radiation Model',fontsize=20)


ax.set_xlabel('$degree, k$',fontsize=30)
ax.set_ylabel('$P(k)$',fontsize=30)
plt.savefig("distributions.eps",dpi=200,bbox_inches='tight')

#Original Data
print("the minimum degree = ",min(list(dict(g.degree()).values())))
print("the maximum degree = ",max(list(dict(g.degree()).values())))
print("the average degree = ",np.mean(list(dict(g.degree()).values())))
print("the standard deviation of degree = ",np.std(list(dict(g.degree()).values())))
print('Average clustering coefficient = ',nx.average_clustering(g))
print('Average shortest path length = ',nx.average_shortest_path_length(g))

#Radiation Data
print("the minimum degree = ",min(list(dict(gr.degree()).values())))
print("the maximum degree = ",max(list(dict(gr.degree()).values())))
print("the average degree = ",np.mean(list(dict(gr.degree()).values())))
print("the standard deviation of degree = ",np.std(list(dict(gr.degree()).values())))
print('Average clustering coefficient = ',nx.average_clustering(gr))
print('Average shortest path length = ',nx.average_shortest_path_length(gr))

